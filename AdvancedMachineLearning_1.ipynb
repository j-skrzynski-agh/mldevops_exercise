{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AML 1\n",
        "\n",
        "## Task 1"
      ],
      "metadata": {
        "id": "8CbBBa3I4-iN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install torchsummary\n",
        "%pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkzwe6JvslwJ",
        "outputId": "3db0206e-2e0b-4f33-81c4-8510e3a17927"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.7)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.18.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWL2C71jxpMJ",
        "outputId": "64910134-5816-475f-b852-de4a45189e2e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjs-agh\u001b[0m (\u001b[33mjs-agh-agh\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZM6cJllX43wK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hx42FEbyRvc",
        "outputId": "4ae9dc78-6b33-4683-82c7-c48d25e9d14a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "\n",
        "# Dataset import as in Ptytorch docs\n",
        "ds_train = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "ds_test = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(ds_train, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(ds_test, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "G-3Bo2dZ5b_9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6e4f303-853a-4ab1-b6a3-2bf1aff863ac"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 13.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 212kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.94MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 20.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the labels from datasetg - taken from kaggle\n",
        "labels = {\n",
        "    0: \"T-shirt\",\n",
        "    1: \"Trouser\",\n",
        "    2: \"Pullover\",\n",
        "    3: \"Dress\",\n",
        "    4: \"Coat\",\n",
        "    5: \"Sandal\",\n",
        "    6: \"Shirt\",\n",
        "    7: \"Sneaker\",\n",
        "    8: \"Bag\",\n",
        "    9: \"Ankle Boot\",\n",
        "}"
      ],
      "metadata": {
        "id": "f9kQYLuN5pla"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting the right device to perform the computations on\n",
        "device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBvf_kPA5wKo",
        "outputId": "596c8895-dc08-45ce-8518-a3e6701666bd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definition of the model\n",
        "\n",
        "\n",
        "class FashionMnistClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FashionMnistClassifier, self).__init__()\n",
        "        self.conv_block_1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        self.drop1 = nn.Dropout2d(0.25)\n",
        "\n",
        "        self.conv_block_2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        self.drop2 = nn.Dropout2d(0.25)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(7 * 7 * 64, 512), nn.ReLU(), nn.Linear(512, 512), nn.ReLU(), nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv_block_1(x)\n",
        "        out = self.drop1(out)\n",
        "        out = self.conv_block_2(out)\n",
        "        out = self.drop2(out)\n",
        "        out = self.linear_relu_stack(self.flatten(out))\n",
        "        return out"
      ],
      "metadata": {
        "id": "-INniC6x6Go-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FashionMnistClassifier().to(device)\n",
        "print(model.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDW6qQt48SxN",
        "outputId": "ae36ca59-c1c6-48c3-ecaf-ddfae90f980a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<generator object Module.parameters at 0x7cde71ae8ba0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Settings\n",
        "learning_rate = 1e-3\n",
        "epochs = 5\n",
        "\n",
        "loss = nn.CrossEntropyLoss()  # nn.MSELoss()\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "K3qISP-E8j5C"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_epoch(dataloader, model, loss, opt):\n",
        "    model.train()  # Setting model into train mode\n",
        "    samples = len(dataloader) * dataloader.batch_size\n",
        "    processed = 0\n",
        "    batch_num = 0\n",
        "\n",
        "    for data, label in dataloader:\n",
        "        data = data.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        opt.zero_grad()\n",
        "\n",
        "        predictions = model(data)\n",
        "        loss_result = loss(predictions, label)\n",
        "        loss_result.backward()\n",
        "\n",
        "        opt.step()\n",
        "\n",
        "        processed += len(data)\n",
        "        if batch_num % 100 == 0:\n",
        "            cur_loss = loss_result.item()\n",
        "            print(f\"loss:{cur_loss:>7f} [{processed:>5d}/{samples:>5d}]\")\n",
        "\n",
        "        batch_num += 1"
      ],
      "metadata": {
        "id": "PvncQ4Pn8jwg"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(dataloader, model, loss):\n",
        "    samples = len(dataloader.dataset)\n",
        "    n_batches = len(dataloader)\n",
        "    model.eval()  # Disenables storing training data\n",
        "\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, label in dataloader:\n",
        "            data = data.to(device)\n",
        "            label = label.to(device)\n",
        "\n",
        "            predictions = model(data)\n",
        "\n",
        "            loss_val = loss(predictions, label).item()\n",
        "            correct += (predictions.argmax(1) == label).type(torch.float).sum().item()\n",
        "            test_loss += loss_val\n",
        "    test_loss /= n_batches\n",
        "    accuracy = correct / samples\n",
        "\n",
        "    return accuracy, test_loss, correct  # , cm\n",
        "\n",
        "\n",
        "#     correct = 0\n",
        "#     with torch.no_grad():\n",
        "#         for data, label in dataloader:\n",
        "#             data, label = data.to(device), label.to(device)\n",
        "#             predictions = model(data)\n",
        "#             correct += (predictions.argmax(1) == label).type(torch.float).sum().item()\n",
        "#     accuracy = correct / len(dataloader.dataset)"
      ],
      "metadata": {
        "id": "LgZX39Ad_P2m"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_training(train, test, model, loss, opt, epochs):\n",
        "\n",
        "    wandb.init(\n",
        "      # Set the project where this run will be logged\n",
        "      project=\"basic-intro\",\n",
        "      # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
        "      name=f\"experiment\",\n",
        "      # Track hyperparameters and run metadata\n",
        "      config={\n",
        "      # \"learning_rate\": 0.02,\n",
        "      \"architecture\": \"MNIST\",\n",
        "      \"dataset\": \"CIFAR-100\",\n",
        "      \"epochs\": 10,\n",
        "      })\n",
        "\n",
        "    for e in range(epochs):\n",
        "        print(f\"# Epoch {e}\")\n",
        "        training_epoch(train, model, loss, opt)\n",
        "        train_accuracy, train_loss, train_correct = evaluate(train, model, loss)\n",
        "        test_accuracy, test_loss, test_correct = evaluate(test, model, loss)\n",
        "        print(\n",
        "            f\"train_loss = {train_loss}, train_acc = {train_accuracy}, test_loss = {test_loss}, test_acc = {test_accuracy}\"\n",
        "        )\n",
        "        wandb.log({\"acc\": test_accuracy, \"loss\": test_loss})"
      ],
      "metadata": {
        "id": "xjZk723A-t1c"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "summary(model, input_size=(1, 28, 28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDdP1-MdsTUD",
        "outputId": "e8fc451e-f04c-42b5-b832-43609cfea713"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 28, 28]             320\n",
            "       BatchNorm2d-2           [-1, 32, 28, 28]              64\n",
            "              ReLU-3           [-1, 32, 28, 28]               0\n",
            "         MaxPool2d-4           [-1, 32, 14, 14]               0\n",
            "         Dropout2d-5           [-1, 32, 14, 14]               0\n",
            "            Conv2d-6           [-1, 64, 14, 14]          18,496\n",
            "       BatchNorm2d-7           [-1, 64, 14, 14]             128\n",
            "              ReLU-8           [-1, 64, 14, 14]               0\n",
            "         MaxPool2d-9             [-1, 64, 7, 7]               0\n",
            "        Dropout2d-10             [-1, 64, 7, 7]               0\n",
            "          Flatten-11                 [-1, 3136]               0\n",
            "           Linear-12                  [-1, 512]       1,606,144\n",
            "             ReLU-13                  [-1, 512]               0\n",
            "           Linear-14                  [-1, 512]         262,656\n",
            "             ReLU-15                  [-1, 512]               0\n",
            "           Linear-16                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 1,892,938\n",
            "Trainable params: 1,892,938\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 1.04\n",
            "Params size (MB): 7.22\n",
            "Estimated Total Size (MB): 8.27\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_training(train_loader, test_loader, model, loss, optimizer, 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EExLE-6BEVme",
        "outputId": "b10bc98c-09f3-49f0-dc06-ca496fefc6a7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241119_002120-yg7b10eg</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/js-agh-agh/basic-intro/runs/yg7b10eg' target=\"_blank\">experiment</a></strong> to <a href='https://wandb.ai/js-agh-agh/basic-intro' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/js-agh-agh/basic-intro' target=\"_blank\">https://wandb.ai/js-agh-agh/basic-intro</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/js-agh-agh/basic-intro/runs/yg7b10eg' target=\"_blank\">https://wandb.ai/js-agh-agh/basic-intro/runs/yg7b10eg</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Epoch 0\n",
            "loss:2.281497 [   32/60000]\n",
            "loss:2.282979 [ 3232/60000]\n",
            "loss:2.163255 [ 6432/60000]\n",
            "loss:2.105489 [ 9632/60000]\n",
            "loss:1.963551 [12832/60000]\n",
            "loss:1.809404 [16032/60000]\n",
            "loss:1.593206 [19232/60000]\n",
            "loss:1.432822 [22432/60000]\n",
            "loss:1.377826 [25632/60000]\n",
            "loss:1.264675 [28832/60000]\n",
            "loss:1.115027 [32032/60000]\n",
            "loss:1.059844 [35232/60000]\n",
            "loss:1.051311 [38432/60000]\n",
            "loss:1.152197 [41632/60000]\n",
            "loss:1.136673 [44832/60000]\n",
            "loss:0.900564 [48032/60000]\n",
            "loss:0.755875 [51232/60000]\n",
            "loss:0.979309 [54432/60000]\n",
            "loss:0.868018 [57632/60000]\n",
            "train_loss = 0.7297321080048879, train_acc = 0.7555666666666667, test_loss = 0.7351396874117013, test_acc = 0.7507\n",
            "# Epoch 1\n",
            "loss:0.812381 [   32/60000]\n",
            "loss:0.939322 [ 3232/60000]\n",
            "loss:0.771545 [ 6432/60000]\n",
            "loss:0.709850 [ 9632/60000]\n",
            "loss:0.857413 [12832/60000]\n",
            "loss:0.921410 [16032/60000]\n",
            "loss:0.515961 [19232/60000]\n",
            "loss:0.767361 [22432/60000]\n",
            "loss:0.723205 [25632/60000]\n",
            "loss:0.544059 [28832/60000]\n",
            "loss:0.692607 [32032/60000]\n",
            "loss:0.557623 [35232/60000]\n",
            "loss:0.661141 [38432/60000]\n",
            "loss:0.615799 [41632/60000]\n",
            "loss:0.345792 [44832/60000]\n",
            "loss:0.932963 [48032/60000]\n",
            "loss:0.562945 [51232/60000]\n",
            "loss:0.517607 [54432/60000]\n",
            "loss:0.679718 [57632/60000]\n",
            "train_loss = 0.543890716346105, train_acc = 0.8032166666666667, test_loss = 0.5556751115443989, test_acc = 0.7936\n",
            "# Epoch 2\n",
            "loss:0.528309 [   32/60000]\n",
            "loss:0.598884 [ 3232/60000]\n",
            "loss:0.751312 [ 6432/60000]\n",
            "loss:0.385164 [ 9632/60000]\n",
            "loss:0.523764 [12832/60000]\n",
            "loss:0.623398 [16032/60000]\n",
            "loss:0.364974 [19232/60000]\n",
            "loss:0.700052 [22432/60000]\n",
            "loss:0.552921 [25632/60000]\n",
            "loss:0.577991 [28832/60000]\n",
            "loss:0.529737 [32032/60000]\n",
            "loss:0.494957 [35232/60000]\n",
            "loss:0.548447 [38432/60000]\n",
            "loss:0.772728 [41632/60000]\n",
            "loss:0.539369 [44832/60000]\n",
            "loss:0.457350 [48032/60000]\n",
            "loss:0.335435 [51232/60000]\n",
            "loss:0.388231 [54432/60000]\n",
            "loss:0.485478 [57632/60000]\n",
            "train_loss = 0.47457918667793275, train_acc = 0.8289166666666666, test_loss = 0.4878784001063996, test_acc = 0.8208\n",
            "# Epoch 3\n",
            "loss:0.310489 [   32/60000]\n",
            "loss:0.536094 [ 3232/60000]\n",
            "loss:0.621204 [ 6432/60000]\n",
            "loss:0.439347 [ 9632/60000]\n",
            "loss:0.552476 [12832/60000]\n",
            "loss:0.505540 [16032/60000]\n",
            "loss:0.470031 [19232/60000]\n",
            "loss:0.431640 [22432/60000]\n",
            "loss:0.531725 [25632/60000]\n",
            "loss:0.325449 [28832/60000]\n",
            "loss:0.321591 [32032/60000]\n",
            "loss:0.394713 [35232/60000]\n",
            "loss:0.578600 [38432/60000]\n",
            "loss:0.504280 [41632/60000]\n",
            "loss:0.379748 [44832/60000]\n",
            "loss:0.427319 [48032/60000]\n",
            "loss:0.371589 [51232/60000]\n",
            "loss:0.500278 [54432/60000]\n",
            "loss:0.601090 [57632/60000]\n",
            "train_loss = 0.4334827231049538, train_acc = 0.8465833333333334, test_loss = 0.4490956329404356, test_acc = 0.8375\n",
            "# Epoch 4\n",
            "loss:0.445291 [   32/60000]\n",
            "loss:0.576341 [ 3232/60000]\n",
            "loss:0.399846 [ 6432/60000]\n",
            "loss:0.506502 [ 9632/60000]\n",
            "loss:0.499790 [12832/60000]\n",
            "loss:0.669152 [16032/60000]\n",
            "loss:0.641113 [19232/60000]\n",
            "loss:0.338705 [22432/60000]\n",
            "loss:0.659533 [25632/60000]\n",
            "loss:0.630635 [28832/60000]\n",
            "loss:0.284392 [32032/60000]\n",
            "loss:0.251375 [35232/60000]\n",
            "loss:0.245083 [38432/60000]\n",
            "loss:0.267263 [41632/60000]\n",
            "loss:0.324641 [44832/60000]\n",
            "loss:0.555061 [48032/60000]\n",
            "loss:0.358081 [51232/60000]\n",
            "loss:0.282267 [54432/60000]\n",
            "loss:0.655853 [57632/60000]\n",
            "train_loss = 0.40172988104422885, train_acc = 0.85795, test_loss = 0.41980745322026386, test_acc = 0.8507\n",
            "# Epoch 5\n",
            "loss:0.475904 [   32/60000]\n",
            "loss:0.504732 [ 3232/60000]\n",
            "loss:0.413141 [ 6432/60000]\n",
            "loss:0.370057 [ 9632/60000]\n",
            "loss:0.373135 [12832/60000]\n",
            "loss:0.493755 [16032/60000]\n",
            "loss:0.349897 [19232/60000]\n",
            "loss:0.242276 [22432/60000]\n",
            "loss:0.513274 [25632/60000]\n",
            "loss:0.488833 [28832/60000]\n",
            "loss:0.402087 [32032/60000]\n",
            "loss:0.339949 [35232/60000]\n",
            "loss:0.206293 [38432/60000]\n",
            "loss:0.369014 [41632/60000]\n",
            "loss:0.327219 [44832/60000]\n",
            "loss:0.410614 [48032/60000]\n",
            "loss:0.318670 [51232/60000]\n",
            "loss:0.476883 [54432/60000]\n",
            "loss:0.500599 [57632/60000]\n",
            "train_loss = 0.37997283945878346, train_acc = 0.8651666666666666, test_loss = 0.4000591697117772, test_acc = 0.8547\n",
            "# Epoch 6\n",
            "loss:0.372546 [   32/60000]\n",
            "loss:0.515224 [ 3232/60000]\n",
            "loss:0.308926 [ 6432/60000]\n",
            "loss:0.350222 [ 9632/60000]\n",
            "loss:0.577516 [12832/60000]\n",
            "loss:0.435638 [16032/60000]\n",
            "loss:0.344924 [19232/60000]\n",
            "loss:0.238207 [22432/60000]\n",
            "loss:0.240670 [25632/60000]\n",
            "loss:0.531938 [28832/60000]\n",
            "loss:0.335971 [32032/60000]\n",
            "loss:0.246288 [35232/60000]\n",
            "loss:0.380943 [38432/60000]\n",
            "loss:0.545123 [41632/60000]\n",
            "loss:0.336216 [44832/60000]\n",
            "loss:0.527546 [48032/60000]\n",
            "loss:0.282875 [51232/60000]\n",
            "loss:0.136119 [54432/60000]\n",
            "loss:0.318128 [57632/60000]\n",
            "train_loss = 0.37043046946922936, train_acc = 0.86705, test_loss = 0.3917418243452764, test_acc = 0.8587\n",
            "# Epoch 7\n",
            "loss:0.351630 [   32/60000]\n",
            "loss:0.273832 [ 3232/60000]\n",
            "loss:0.510008 [ 6432/60000]\n",
            "loss:0.577569 [ 9632/60000]\n",
            "loss:0.371955 [12832/60000]\n",
            "loss:0.537107 [16032/60000]\n",
            "loss:0.427999 [19232/60000]\n",
            "loss:0.616690 [22432/60000]\n",
            "loss:0.127691 [25632/60000]\n",
            "loss:0.337072 [28832/60000]\n",
            "loss:0.331940 [32032/60000]\n",
            "loss:0.402907 [35232/60000]\n",
            "loss:0.336194 [38432/60000]\n",
            "loss:0.363190 [41632/60000]\n",
            "loss:0.471489 [44832/60000]\n",
            "loss:0.198377 [48032/60000]\n",
            "loss:0.307375 [51232/60000]\n",
            "loss:0.480295 [54432/60000]\n",
            "loss:0.199472 [57632/60000]\n",
            "train_loss = 0.356242435558637, train_acc = 0.8718833333333333, test_loss = 0.3777063648445538, test_acc = 0.864\n",
            "# Epoch 8\n",
            "loss:0.287599 [   32/60000]\n",
            "loss:0.589959 [ 3232/60000]\n",
            "loss:0.204783 [ 6432/60000]\n",
            "loss:0.373012 [ 9632/60000]\n",
            "loss:0.445524 [12832/60000]\n",
            "loss:0.345578 [16032/60000]\n",
            "loss:0.386466 [19232/60000]\n",
            "loss:0.550439 [22432/60000]\n",
            "loss:0.331872 [25632/60000]\n",
            "loss:0.428753 [28832/60000]\n",
            "loss:0.305601 [32032/60000]\n",
            "loss:0.200551 [35232/60000]\n",
            "loss:0.189637 [38432/60000]\n",
            "loss:0.148431 [41632/60000]\n",
            "loss:0.535903 [44832/60000]\n",
            "loss:0.152403 [48032/60000]\n",
            "loss:0.446881 [51232/60000]\n",
            "loss:0.602517 [54432/60000]\n",
            "loss:0.393560 [57632/60000]\n",
            "train_loss = 0.33655871660113335, train_acc = 0.8801666666666667, test_loss = 0.3603417841485514, test_acc = 0.8701\n",
            "# Epoch 9\n",
            "loss:0.308905 [   32/60000]\n",
            "loss:0.446405 [ 3232/60000]\n",
            "loss:0.341467 [ 6432/60000]\n",
            "loss:0.470069 [ 9632/60000]\n",
            "loss:0.460930 [12832/60000]\n",
            "loss:0.527858 [16032/60000]\n",
            "loss:0.290803 [19232/60000]\n",
            "loss:0.299588 [22432/60000]\n",
            "loss:0.285890 [25632/60000]\n",
            "loss:0.151952 [28832/60000]\n",
            "loss:0.381712 [32032/60000]\n",
            "loss:0.192622 [35232/60000]\n",
            "loss:0.245302 [38432/60000]\n",
            "loss:0.281231 [41632/60000]\n",
            "loss:0.250081 [44832/60000]\n",
            "loss:0.156449 [48032/60000]\n",
            "loss:0.507184 [51232/60000]\n",
            "loss:0.210822 [54432/60000]\n",
            "loss:0.262736 [57632/60000]\n",
            "train_loss = 0.32525164707899096, train_acc = 0.8846833333333334, test_loss = 0.35210051378026935, test_acc = 0.873\n",
            "# Epoch 10\n",
            "loss:0.209236 [   32/60000]\n",
            "loss:0.279470 [ 3232/60000]\n",
            "loss:0.136564 [ 6432/60000]\n",
            "loss:0.368120 [ 9632/60000]\n",
            "loss:0.239050 [12832/60000]\n",
            "loss:0.169831 [16032/60000]\n",
            "loss:0.308921 [19232/60000]\n",
            "loss:0.148580 [22432/60000]\n",
            "loss:0.558508 [25632/60000]\n",
            "loss:0.173786 [28832/60000]\n",
            "loss:0.252970 [32032/60000]\n",
            "loss:0.378538 [35232/60000]\n",
            "loss:0.397414 [38432/60000]\n",
            "loss:0.597944 [41632/60000]\n",
            "loss:0.445125 [44832/60000]\n",
            "loss:0.268806 [48032/60000]\n",
            "loss:0.287286 [51232/60000]\n",
            "loss:0.240380 [54432/60000]\n",
            "loss:0.247195 [57632/60000]\n",
            "train_loss = 0.3176995481908321, train_acc = 0.88765, test_loss = 0.3456104849331295, test_acc = 0.8772\n",
            "# Epoch 11\n",
            "loss:0.257121 [   32/60000]\n",
            "loss:0.381946 [ 3232/60000]\n",
            "loss:0.436361 [ 6432/60000]\n",
            "loss:0.279075 [ 9632/60000]\n",
            "loss:0.444737 [12832/60000]\n",
            "loss:0.424576 [16032/60000]\n",
            "loss:0.322889 [19232/60000]\n",
            "loss:0.287834 [22432/60000]\n",
            "loss:0.282512 [25632/60000]\n",
            "loss:0.483477 [28832/60000]\n",
            "loss:0.459670 [32032/60000]\n",
            "loss:0.588744 [35232/60000]\n",
            "loss:0.315152 [38432/60000]\n",
            "loss:0.515058 [41632/60000]\n",
            "loss:0.176131 [44832/60000]\n",
            "loss:0.403323 [48032/60000]\n",
            "loss:0.240379 [51232/60000]\n",
            "loss:0.375067 [54432/60000]\n",
            "loss:0.481587 [57632/60000]\n",
            "train_loss = 0.307556493083636, train_acc = 0.8903, test_loss = 0.3361244011349, test_acc = 0.8796\n",
            "# Epoch 12\n",
            "loss:0.354052 [   32/60000]\n",
            "loss:0.501910 [ 3232/60000]\n",
            "loss:0.265882 [ 6432/60000]\n",
            "loss:0.404047 [ 9632/60000]\n",
            "loss:0.233673 [12832/60000]\n",
            "loss:0.376265 [16032/60000]\n",
            "loss:0.261758 [19232/60000]\n",
            "loss:0.383534 [22432/60000]\n",
            "loss:0.264783 [25632/60000]\n",
            "loss:0.322706 [28832/60000]\n",
            "loss:0.215530 [32032/60000]\n",
            "loss:0.176926 [35232/60000]\n",
            "loss:0.475215 [38432/60000]\n",
            "loss:0.144192 [41632/60000]\n",
            "loss:0.559442 [44832/60000]\n",
            "loss:0.407555 [48032/60000]\n",
            "loss:0.273166 [51232/60000]\n",
            "loss:0.309367 [54432/60000]\n",
            "loss:0.477193 [57632/60000]\n",
            "train_loss = 0.3081050148407618, train_acc = 0.8904, test_loss = 0.3372740383727101, test_acc = 0.8804\n",
            "# Epoch 13\n",
            "loss:0.449381 [   32/60000]\n",
            "loss:0.169316 [ 3232/60000]\n",
            "loss:0.137570 [ 6432/60000]\n",
            "loss:0.246743 [ 9632/60000]\n",
            "loss:0.153632 [12832/60000]\n",
            "loss:0.386729 [16032/60000]\n",
            "loss:0.333558 [19232/60000]\n",
            "loss:0.529580 [22432/60000]\n",
            "loss:0.426670 [25632/60000]\n",
            "loss:0.252603 [28832/60000]\n",
            "loss:0.367134 [32032/60000]\n",
            "loss:0.532102 [35232/60000]\n",
            "loss:0.133223 [38432/60000]\n",
            "loss:0.226417 [41632/60000]\n",
            "loss:0.618113 [44832/60000]\n",
            "loss:0.320013 [48032/60000]\n",
            "loss:0.261663 [51232/60000]\n",
            "loss:0.127164 [54432/60000]\n",
            "loss:0.280759 [57632/60000]\n",
            "train_loss = 0.2945671972354253, train_acc = 0.8951666666666667, test_loss = 0.3263058408428305, test_acc = 0.882\n",
            "# Epoch 14\n",
            "loss:0.187945 [   32/60000]\n",
            "loss:0.213782 [ 3232/60000]\n",
            "loss:0.368057 [ 6432/60000]\n",
            "loss:0.282758 [ 9632/60000]\n",
            "loss:0.241428 [12832/60000]\n",
            "loss:0.158313 [16032/60000]\n",
            "loss:0.100407 [19232/60000]\n",
            "loss:0.152050 [22432/60000]\n",
            "loss:0.722773 [25632/60000]\n",
            "loss:0.544085 [28832/60000]\n",
            "loss:0.260903 [32032/60000]\n",
            "loss:0.356391 [35232/60000]\n",
            "loss:0.307870 [38432/60000]\n",
            "loss:0.383533 [41632/60000]\n",
            "loss:0.283784 [44832/60000]\n",
            "loss:0.462856 [48032/60000]\n",
            "loss:0.374451 [51232/60000]\n",
            "loss:0.274839 [54432/60000]\n",
            "loss:0.248941 [57632/60000]\n",
            "train_loss = 0.2862467995782693, train_acc = 0.8978, test_loss = 0.31725716837250384, test_acc = 0.8867\n",
            "# Epoch 15\n",
            "loss:0.210879 [   32/60000]\n",
            "loss:0.475780 [ 3232/60000]\n",
            "loss:0.368603 [ 6432/60000]\n",
            "loss:0.179482 [ 9632/60000]\n",
            "loss:0.313587 [12832/60000]\n",
            "loss:0.417527 [16032/60000]\n",
            "loss:0.252445 [19232/60000]\n",
            "loss:0.771374 [22432/60000]\n",
            "loss:0.515681 [25632/60000]\n",
            "loss:0.313851 [28832/60000]\n",
            "loss:0.090237 [32032/60000]\n",
            "loss:0.353995 [35232/60000]\n",
            "loss:0.361301 [38432/60000]\n",
            "loss:0.237262 [41632/60000]\n",
            "loss:0.239389 [44832/60000]\n",
            "loss:0.349589 [48032/60000]\n",
            "loss:0.264081 [51232/60000]\n",
            "loss:0.444193 [54432/60000]\n",
            "loss:0.174906 [57632/60000]\n",
            "train_loss = 0.2823933815995852, train_acc = 0.8999166666666667, test_loss = 0.31503172512776173, test_acc = 0.887\n",
            "# Epoch 16\n",
            "loss:0.421899 [   32/60000]\n",
            "loss:0.213461 [ 3232/60000]\n",
            "loss:0.204621 [ 6432/60000]\n",
            "loss:0.376876 [ 9632/60000]\n",
            "loss:0.284104 [12832/60000]\n",
            "loss:0.351684 [16032/60000]\n",
            "loss:0.261961 [19232/60000]\n",
            "loss:0.303145 [22432/60000]\n",
            "loss:0.115537 [25632/60000]\n",
            "loss:0.346313 [28832/60000]\n",
            "loss:0.236387 [32032/60000]\n",
            "loss:0.296683 [35232/60000]\n",
            "loss:0.174741 [38432/60000]\n",
            "loss:0.170175 [41632/60000]\n",
            "loss:0.281844 [44832/60000]\n",
            "loss:0.230963 [48032/60000]\n",
            "loss:0.369677 [51232/60000]\n",
            "loss:0.165248 [54432/60000]\n",
            "loss:0.337000 [57632/60000]\n",
            "train_loss = 0.27705226956804596, train_acc = 0.9013666666666666, test_loss = 0.31078171933563753, test_acc = 0.8873\n",
            "# Epoch 17\n",
            "loss:0.188224 [   32/60000]\n",
            "loss:0.320763 [ 3232/60000]\n",
            "loss:0.267943 [ 6432/60000]\n",
            "loss:0.324494 [ 9632/60000]\n",
            "loss:0.526240 [12832/60000]\n",
            "loss:0.236185 [16032/60000]\n",
            "loss:0.432736 [19232/60000]\n",
            "loss:0.362135 [22432/60000]\n",
            "loss:0.923314 [25632/60000]\n",
            "loss:0.695417 [28832/60000]\n",
            "loss:0.183667 [32032/60000]\n",
            "loss:0.487528 [35232/60000]\n",
            "loss:0.337361 [38432/60000]\n",
            "loss:0.363653 [41632/60000]\n",
            "loss:0.142711 [44832/60000]\n",
            "loss:0.184595 [48032/60000]\n",
            "loss:0.326728 [51232/60000]\n",
            "loss:0.317458 [54432/60000]\n",
            "loss:0.410812 [57632/60000]\n",
            "train_loss = 0.2736329393029213, train_acc = 0.90355, test_loss = 0.30904724965461144, test_acc = 0.8903\n",
            "# Epoch 18\n",
            "loss:0.295414 [   32/60000]\n",
            "loss:0.264045 [ 3232/60000]\n",
            "loss:0.196315 [ 6432/60000]\n",
            "loss:0.269077 [ 9632/60000]\n",
            "loss:0.235394 [12832/60000]\n",
            "loss:0.377250 [16032/60000]\n",
            "loss:0.167694 [19232/60000]\n",
            "loss:0.341765 [22432/60000]\n",
            "loss:0.282541 [25632/60000]\n",
            "loss:0.156710 [28832/60000]\n",
            "loss:0.222346 [32032/60000]\n",
            "loss:0.143006 [35232/60000]\n",
            "loss:0.565480 [38432/60000]\n",
            "loss:0.206093 [41632/60000]\n",
            "loss:0.105328 [44832/60000]\n",
            "loss:0.429369 [48032/60000]\n",
            "loss:0.440117 [51232/60000]\n",
            "loss:0.398637 [54432/60000]\n",
            "loss:0.246921 [57632/60000]\n",
            "train_loss = 0.2673932257970174, train_acc = 0.9055, test_loss = 0.3028709754562035, test_acc = 0.8921\n",
            "# Epoch 19\n",
            "loss:0.230969 [   32/60000]\n",
            "loss:0.472126 [ 3232/60000]\n",
            "loss:0.529978 [ 6432/60000]\n",
            "loss:0.348850 [ 9632/60000]\n",
            "loss:0.232902 [12832/60000]\n",
            "loss:0.392023 [16032/60000]\n",
            "loss:0.203340 [19232/60000]\n",
            "loss:0.144452 [22432/60000]\n",
            "loss:0.414922 [25632/60000]\n",
            "loss:0.198002 [28832/60000]\n",
            "loss:0.295438 [32032/60000]\n",
            "loss:0.259929 [35232/60000]\n",
            "loss:0.247862 [38432/60000]\n",
            "loss:0.558614 [41632/60000]\n",
            "loss:0.179256 [44832/60000]\n",
            "loss:0.234724 [48032/60000]\n",
            "loss:0.232460 [51232/60000]\n",
            "loss:0.572655 [54432/60000]\n",
            "loss:0.320324 [57632/60000]\n",
            "train_loss = 0.26102094307343165, train_acc = 0.9073166666666667, test_loss = 0.29840554274356784, test_acc = 0.8926\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2"
      ],
      "metadata": {
        "id": "M27sfUwm-yMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AutoEncoder(torch.nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(AutoEncoder, self).__init__()\n",
        "        self.encoder = torch.nn.Sequential(\n",
        "            torch.nn.Linear(input_size, 128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(128, 64),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(64, 32),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(32, 16),\n",
        "        )\n",
        "\n",
        "        self.decoder = torch.nn.Sequential(\n",
        "            torch.nn.Linear(16, 32),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(32, 64),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(64, 128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(128, input_size),\n",
        "            torch.nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.encoder(x)\n",
        "        out = self.decoder(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "5l8_ucTTCR_a"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train = datasets.FashionMNIST(root=\"data\", train=True, download=True, transform=ToTensor())\n",
        "ds_test = datasets.FashionMNIST(root=\"data\", train=False, download=True, transform=ToTensor())\n",
        "batch_size = 32\n",
        "train_dataloader = DataLoader(ds_train, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(ds_test, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "model = AutoEncoder(28 * 28)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-1, weight_decay=1e-8)\n",
        "\n",
        "loss_f = torch.nn.MSELoss()\n",
        "\n",
        "# Selecting the right device to perform the computations on\n",
        "device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-5phlXRLue4",
        "outputId": "e4a46b76-90cb-40a4-a5b9-2dd5edc75497"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def one_epoch_train(model, optimizer, loss, train_dataloader, device):\n",
        "    model.train()\n",
        "    processed_images = 0\n",
        "    current_batch_index = 0\n",
        "    samples = len(train_dataloader.dataset)\n",
        "    for image, _ in train_dataloader:\n",
        "        image = image.to(device)\n",
        "        image = image.reshape(-1, 28 * 28)\n",
        "\n",
        "        reconstructed_image = model(image)\n",
        "        computed_loss = loss(reconstructed_image, image)\n",
        "        computed_loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        processed_images += len(image)\n",
        "        if current_batch_index % 100 == 0:\n",
        "            current_loss_value = computed_loss.item()\n",
        "            print(f\"loss:{current_loss_value:>7f} [{processed_images:>5d}/{samples:>5d}]\")\n",
        "        current_batch_index += 1"
      ],
      "metadata": {
        "id": "YWnKX8drL8uN"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, loss, dataloader, device):\n",
        "    model.eval()\n",
        "    samples = len(dataloader.dataset)\n",
        "    n_batches = len(dataloader)\n",
        "    total_loss = 0\n",
        "    samples = len(dataloader.dataset)\n",
        "    for image, _ in dataloader:\n",
        "        image = image.to(device)\n",
        "        image = image.reshape(-1, 28 * 28)\n",
        "\n",
        "        reconstructed_image = model(image)\n",
        "        computed_loss = loss(reconstructed_image, image)\n",
        "        total_loss += computed_loss.item()\n",
        "    return total_loss / n_batches"
      ],
      "metadata": {
        "id": "VqXrXKP4FYvz"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_training(train, test, model, loss, opt, epochs):\n",
        "    wandb.init(\n",
        "      # Set the project where this run will be logged\n",
        "      project=\"task2-autoencoder\",\n",
        "      # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
        "      name=f\"experiment\",\n",
        "      # Track hyperparameters and run metadata\n",
        "      config={\n",
        "      # \"learning_rate\": 0.02,\n",
        "      \"architecture\": \"Autoencoder\",\n",
        "      \"dataset\": \"FashionMnist\",\n",
        "      \"epochs\": 10,\n",
        "      })\n",
        "    for e in range(epochs):\n",
        "        print(f\"# Epoch {e}\")\n",
        "        one_epoch_train(model, opt, loss, train, device)\n",
        "        train_loss = evaluate(model, loss, train, device)\n",
        "        test_loss = evaluate(model, loss, test, device)\n",
        "        print(f\"train_loss = {train_loss}, test_loss = {test_loss}\")\n",
        "        wandb.log({\"loss\": test_loss})\n",
        "\n",
        "\n",
        "run_training(train_dataloader, test_dataloader, model, loss_f, optimizer, 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KB_Fyu8XOy4u",
        "outputId": "c11aeecc-7821-4a5b-bed6-cdad8c7f1483"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:yqtchgft) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">experiment</strong> at: <a href='https://wandb.ai/js-agh-agh/task2-autoencoder/runs/yqtchgft' target=\"_blank\">https://wandb.ai/js-agh-agh/task2-autoencoder/runs/yqtchgft</a><br/> View project at: <a href='https://wandb.ai/js-agh-agh/task2-autoencoder' target=\"_blank\">https://wandb.ai/js-agh-agh/task2-autoencoder</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20241119_003404-yqtchgft/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:yqtchgft). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241119_003542-bq4u02y0</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/js-agh-agh/task2-autoencoder/runs/bq4u02y0' target=\"_blank\">experiment</a></strong> to <a href='https://wandb.ai/js-agh-agh/task2-autoencoder' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/js-agh-agh/task2-autoencoder' target=\"_blank\">https://wandb.ai/js-agh-agh/task2-autoencoder</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/js-agh-agh/task2-autoencoder/runs/bq4u02y0' target=\"_blank\">https://wandb.ai/js-agh-agh/task2-autoencoder/runs/bq4u02y0</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Epoch 0\n",
            "loss:0.216613 [   32/60000]\n",
            "loss:0.231600 [ 3232/60000]\n",
            "loss:0.217344 [ 6432/60000]\n",
            "loss:0.209716 [ 9632/60000]\n",
            "loss:0.196034 [12832/60000]\n",
            "loss:0.206638 [16032/60000]\n",
            "loss:0.181775 [19232/60000]\n",
            "loss:0.196764 [22432/60000]\n",
            "loss:0.227923 [25632/60000]\n",
            "loss:0.209154 [28832/60000]\n",
            "loss:0.207005 [32032/60000]\n",
            "loss:0.210441 [35232/60000]\n",
            "loss:0.201638 [38432/60000]\n",
            "loss:0.212891 [41632/60000]\n",
            "loss:0.211371 [44832/60000]\n",
            "loss:0.202502 [48032/60000]\n",
            "loss:0.195790 [51232/60000]\n",
            "loss:0.211923 [54432/60000]\n",
            "loss:0.219100 [57632/60000]\n",
            "train_loss = 0.20644534362951913, test_loss = 0.20640322808830883\n",
            "# Epoch 1\n",
            "loss:0.171770 [   32/60000]\n",
            "loss:0.220140 [ 3232/60000]\n",
            "loss:0.219624 [ 6432/60000]\n",
            "loss:0.200138 [ 9632/60000]\n",
            "loss:0.211830 [12832/60000]\n",
            "loss:0.192850 [16032/60000]\n",
            "loss:0.182935 [19232/60000]\n",
            "loss:0.196765 [22432/60000]\n",
            "loss:0.169959 [25632/60000]\n",
            "loss:0.190635 [28832/60000]\n",
            "loss:0.210318 [32032/60000]\n",
            "loss:0.221196 [35232/60000]\n",
            "loss:0.207850 [38432/60000]\n",
            "loss:0.194765 [41632/60000]\n",
            "loss:0.261929 [44832/60000]\n",
            "loss:0.224715 [48032/60000]\n",
            "loss:0.234861 [51232/60000]\n",
            "loss:0.167479 [54432/60000]\n",
            "loss:0.234723 [57632/60000]\n",
            "train_loss = 0.20644534362157185, test_loss = 0.20657838742953902\n",
            "# Epoch 2\n",
            "loss:0.199349 [   32/60000]\n",
            "loss:0.214060 [ 3232/60000]\n",
            "loss:0.196100 [ 6432/60000]\n",
            "loss:0.186078 [ 9632/60000]\n",
            "loss:0.187919 [12832/60000]\n",
            "loss:0.202628 [16032/60000]\n",
            "loss:0.221514 [19232/60000]\n",
            "loss:0.191709 [22432/60000]\n",
            "loss:0.207998 [25632/60000]\n",
            "loss:0.177440 [28832/60000]\n",
            "loss:0.204363 [32032/60000]\n",
            "loss:0.220369 [35232/60000]\n",
            "loss:0.188336 [38432/60000]\n",
            "loss:0.215948 [41632/60000]\n",
            "loss:0.221218 [44832/60000]\n",
            "loss:0.201850 [48032/60000]\n",
            "loss:0.198877 [51232/60000]\n",
            "loss:0.168937 [54432/60000]\n",
            "loss:0.177444 [57632/60000]\n",
            "train_loss = 0.20644534374078116, test_loss = 0.20644219671956265\n",
            "# Epoch 3\n",
            "loss:0.188204 [   32/60000]\n",
            "loss:0.187656 [ 3232/60000]\n",
            "loss:0.230561 [ 6432/60000]\n",
            "loss:0.182740 [ 9632/60000]\n",
            "loss:0.235290 [12832/60000]\n",
            "loss:0.227307 [16032/60000]\n",
            "loss:0.190757 [19232/60000]\n",
            "loss:0.238644 [22432/60000]\n",
            "loss:0.209248 [25632/60000]\n",
            "loss:0.208174 [28832/60000]\n",
            "loss:0.182016 [32032/60000]\n",
            "loss:0.166431 [35232/60000]\n",
            "loss:0.242912 [38432/60000]\n",
            "loss:0.248813 [41632/60000]\n",
            "loss:0.195698 [44832/60000]\n",
            "loss:0.214649 [48032/60000]\n",
            "loss:0.195272 [51232/60000]\n",
            "loss:0.225407 [54432/60000]\n",
            "loss:0.195949 [57632/60000]\n",
            "train_loss = 0.20644534324010214, test_loss = 0.20655639760029582\n",
            "# Epoch 4\n",
            "loss:0.204823 [   32/60000]\n",
            "loss:0.146561 [ 3232/60000]\n",
            "loss:0.191724 [ 6432/60000]\n",
            "loss:0.214519 [ 9632/60000]\n",
            "loss:0.213939 [12832/60000]\n",
            "loss:0.244399 [16032/60000]\n",
            "loss:0.256339 [19232/60000]\n",
            "loss:0.218782 [22432/60000]\n",
            "loss:0.247671 [25632/60000]\n",
            "loss:0.203150 [28832/60000]\n",
            "loss:0.209284 [32032/60000]\n",
            "loss:0.220202 [35232/60000]\n",
            "loss:0.224461 [38432/60000]\n",
            "loss:0.165272 [41632/60000]\n",
            "loss:0.214855 [44832/60000]\n",
            "loss:0.203452 [48032/60000]\n",
            "loss:0.184839 [51232/60000]\n",
            "loss:0.188773 [54432/60000]\n",
            "loss:0.226712 [57632/60000]\n",
            "train_loss = 0.20644534364541373, test_loss = 0.20653860808942265\n",
            "# Epoch 5\n",
            "loss:0.205859 [   32/60000]\n",
            "loss:0.217001 [ 3232/60000]\n",
            "loss:0.194850 [ 6432/60000]\n",
            "loss:0.208490 [ 9632/60000]\n",
            "loss:0.231764 [12832/60000]\n",
            "loss:0.224309 [16032/60000]\n",
            "loss:0.191899 [19232/60000]\n",
            "loss:0.196828 [22432/60000]\n",
            "loss:0.184603 [25632/60000]\n",
            "loss:0.258805 [28832/60000]\n",
            "loss:0.194159 [32032/60000]\n",
            "loss:0.223923 [35232/60000]\n",
            "loss:0.208191 [38432/60000]\n",
            "loss:0.174042 [41632/60000]\n",
            "loss:0.203378 [44832/60000]\n",
            "loss:0.198618 [48032/60000]\n",
            "loss:0.210595 [51232/60000]\n",
            "loss:0.168778 [54432/60000]\n",
            "loss:0.242234 [57632/60000]\n",
            "train_loss = 0.20644534380435944, test_loss = 0.20646009372826962\n",
            "# Epoch 6\n",
            "loss:0.227018 [   32/60000]\n",
            "loss:0.204161 [ 3232/60000]\n",
            "loss:0.185134 [ 6432/60000]\n",
            "loss:0.201812 [ 9632/60000]\n",
            "loss:0.235733 [12832/60000]\n",
            "loss:0.191670 [16032/60000]\n",
            "loss:0.224900 [19232/60000]\n",
            "loss:0.213804 [22432/60000]\n",
            "loss:0.197123 [25632/60000]\n",
            "loss:0.216150 [28832/60000]\n",
            "loss:0.230440 [32032/60000]\n",
            "loss:0.210554 [35232/60000]\n",
            "loss:0.206474 [38432/60000]\n",
            "loss:0.215539 [41632/60000]\n",
            "loss:0.236381 [44832/60000]\n",
            "loss:0.175362 [48032/60000]\n",
            "loss:0.217614 [51232/60000]\n",
            "loss:0.205010 [54432/60000]\n",
            "loss:0.196526 [57632/60000]\n",
            "train_loss = 0.20644534358183542, test_loss = 0.20641863507965502\n",
            "# Epoch 7\n",
            "loss:0.217393 [   32/60000]\n",
            "loss:0.220203 [ 3232/60000]\n",
            "loss:0.219607 [ 6432/60000]\n",
            "loss:0.180903 [ 9632/60000]\n",
            "loss:0.189179 [12832/60000]\n",
            "loss:0.179999 [16032/60000]\n",
            "loss:0.201392 [19232/60000]\n",
            "loss:0.257276 [22432/60000]\n",
            "loss:0.217936 [25632/60000]\n",
            "loss:0.214209 [28832/60000]\n",
            "loss:0.209228 [32032/60000]\n",
            "loss:0.241638 [35232/60000]\n",
            "loss:0.184494 [38432/60000]\n",
            "loss:0.194153 [41632/60000]\n",
            "loss:0.194481 [44832/60000]\n",
            "loss:0.183826 [48032/60000]\n",
            "loss:0.204623 [51232/60000]\n",
            "loss:0.223547 [54432/60000]\n",
            "loss:0.222505 [57632/60000]\n",
            "train_loss = 0.20644534333546957, test_loss = 0.20650607928300438\n",
            "# Epoch 8\n",
            "loss:0.187497 [   32/60000]\n",
            "loss:0.211034 [ 3232/60000]\n",
            "loss:0.219182 [ 6432/60000]\n",
            "loss:0.207198 [ 9632/60000]\n",
            "loss:0.244373 [12832/60000]\n",
            "loss:0.217065 [16032/60000]\n",
            "loss:0.222247 [19232/60000]\n",
            "loss:0.245947 [22432/60000]\n",
            "loss:0.184651 [25632/60000]\n",
            "loss:0.260902 [28832/60000]\n",
            "loss:0.191086 [32032/60000]\n",
            "loss:0.193718 [35232/60000]\n",
            "loss:0.207623 [38432/60000]\n",
            "loss:0.195104 [41632/60000]\n",
            "loss:0.206364 [44832/60000]\n",
            "loss:0.207931 [48032/60000]\n",
            "loss:0.173530 [51232/60000]\n",
            "loss:0.220842 [54432/60000]\n",
            "loss:0.179939 [57632/60000]\n",
            "train_loss = 0.206445343875885, test_loss = 0.20646969017129355\n",
            "# Epoch 9\n",
            "loss:0.210864 [   32/60000]\n",
            "loss:0.210503 [ 3232/60000]\n",
            "loss:0.216716 [ 6432/60000]\n",
            "loss:0.193780 [ 9632/60000]\n",
            "loss:0.187850 [12832/60000]\n",
            "loss:0.228348 [16032/60000]\n",
            "loss:0.224611 [19232/60000]\n",
            "loss:0.189939 [22432/60000]\n",
            "loss:0.219380 [25632/60000]\n",
            "loss:0.193534 [28832/60000]\n",
            "loss:0.221713 [32032/60000]\n",
            "loss:0.226039 [35232/60000]\n",
            "loss:0.207067 [38432/60000]\n",
            "loss:0.235606 [41632/60000]\n",
            "loss:0.195761 [44832/60000]\n",
            "loss:0.187733 [48032/60000]\n",
            "loss:0.221488 [51232/60000]\n",
            "loss:0.219360 [54432/60000]\n",
            "loss:0.205788 [57632/60000]\n",
            "train_loss = 0.20644534351825714, test_loss = 0.2064821228337364\n",
            "# Epoch 10\n",
            "loss:0.243013 [   32/60000]\n",
            "loss:0.204300 [ 3232/60000]\n",
            "loss:0.224953 [ 6432/60000]\n",
            "loss:0.182851 [ 9632/60000]\n",
            "loss:0.187736 [12832/60000]\n",
            "loss:0.183401 [16032/60000]\n",
            "loss:0.242970 [19232/60000]\n",
            "loss:0.228812 [22432/60000]\n",
            "loss:0.185212 [25632/60000]\n",
            "loss:0.193753 [28832/60000]\n",
            "loss:0.230980 [32032/60000]\n",
            "loss:0.220962 [35232/60000]\n",
            "loss:0.225925 [38432/60000]\n",
            "loss:0.196034 [41632/60000]\n",
            "loss:0.208112 [44832/60000]\n",
            "loss:0.216868 [48032/60000]\n",
            "loss:0.219230 [51232/60000]\n",
            "loss:0.183641 [54432/60000]\n",
            "loss:0.222003 [57632/60000]\n",
            "train_loss = 0.206445343430837, test_loss = 0.20642618197031296\n",
            "# Epoch 11\n",
            "loss:0.228298 [   32/60000]\n",
            "loss:0.232109 [ 3232/60000]\n",
            "loss:0.198797 [ 6432/60000]\n",
            "loss:0.205517 [ 9632/60000]\n",
            "loss:0.210645 [12832/60000]\n",
            "loss:0.213448 [16032/60000]\n",
            "loss:0.212221 [19232/60000]\n",
            "loss:0.177956 [22432/60000]\n",
            "loss:0.220138 [25632/60000]\n",
            "loss:0.179394 [28832/60000]\n",
            "loss:0.197657 [32032/60000]\n",
            "loss:0.205510 [35232/60000]\n",
            "loss:0.192660 [38432/60000]\n",
            "loss:0.192381 [41632/60000]\n",
            "loss:0.171831 [44832/60000]\n",
            "loss:0.157629 [48032/60000]\n",
            "loss:0.245276 [51232/60000]\n",
            "loss:0.196900 [54432/60000]\n",
            "loss:0.163102 [57632/60000]\n",
            "train_loss = 0.2064453434785207, test_loss = 0.2064659261284545\n",
            "# Epoch 12\n",
            "loss:0.204361 [   32/60000]\n",
            "loss:0.183935 [ 3232/60000]\n",
            "loss:0.199700 [ 6432/60000]\n",
            "loss:0.222760 [ 9632/60000]\n",
            "loss:0.196748 [12832/60000]\n",
            "loss:0.204532 [16032/60000]\n",
            "loss:0.168070 [19232/60000]\n",
            "loss:0.201778 [22432/60000]\n",
            "loss:0.221423 [25632/60000]\n",
            "loss:0.229106 [28832/60000]\n",
            "loss:0.202724 [32032/60000]\n",
            "loss:0.204710 [35232/60000]\n",
            "loss:0.207704 [38432/60000]\n",
            "loss:0.217144 [41632/60000]\n",
            "loss:0.216691 [44832/60000]\n",
            "loss:0.245995 [48032/60000]\n",
            "loss:0.219801 [51232/60000]\n",
            "loss:0.216293 [54432/60000]\n",
            "loss:0.201336 [57632/60000]\n",
            "train_loss = 0.20644534386793773, test_loss = 0.2065253779054069\n",
            "# Epoch 13\n",
            "loss:0.203523 [   32/60000]\n",
            "loss:0.211104 [ 3232/60000]\n",
            "loss:0.221588 [ 6432/60000]\n",
            "loss:0.217290 [ 9632/60000]\n",
            "loss:0.220162 [12832/60000]\n",
            "loss:0.194146 [16032/60000]\n",
            "loss:0.212017 [19232/60000]\n",
            "loss:0.219079 [22432/60000]\n",
            "loss:0.191015 [25632/60000]\n",
            "loss:0.197543 [28832/60000]\n",
            "loss:0.233829 [32032/60000]\n",
            "loss:0.239938 [35232/60000]\n",
            "loss:0.234380 [38432/60000]\n",
            "loss:0.210137 [41632/60000]\n",
            "loss:0.211943 [44832/60000]\n",
            "loss:0.195528 [48032/60000]\n",
            "loss:0.213603 [51232/60000]\n",
            "loss:0.172526 [54432/60000]\n",
            "loss:0.210358 [57632/60000]\n",
            "train_loss = 0.20644534339110057, test_loss = 0.20645038440775948\n",
            "# Epoch 14\n",
            "loss:0.186282 [   32/60000]\n",
            "loss:0.198102 [ 3232/60000]\n",
            "loss:0.199292 [ 6432/60000]\n",
            "loss:0.202732 [ 9632/60000]\n",
            "loss:0.172145 [12832/60000]\n",
            "loss:0.198897 [16032/60000]\n",
            "loss:0.216933 [19232/60000]\n",
            "loss:0.188064 [22432/60000]\n",
            "loss:0.194862 [25632/60000]\n",
            "loss:0.196878 [28832/60000]\n",
            "loss:0.204336 [32032/60000]\n",
            "loss:0.175436 [35232/60000]\n",
            "loss:0.185653 [38432/60000]\n",
            "loss:0.189985 [41632/60000]\n",
            "loss:0.190848 [44832/60000]\n",
            "loss:0.194372 [48032/60000]\n",
            "loss:0.217992 [51232/60000]\n",
            "loss:0.197910 [54432/60000]\n",
            "loss:0.175571 [57632/60000]\n",
            "train_loss = 0.20644534356594085, test_loss = 0.20647868714012657\n",
            "# Epoch 15\n",
            "loss:0.221082 [   32/60000]\n",
            "loss:0.214294 [ 3232/60000]\n",
            "loss:0.198392 [ 6432/60000]\n",
            "loss:0.218560 [ 9632/60000]\n",
            "loss:0.178276 [12832/60000]\n",
            "loss:0.196960 [16032/60000]\n",
            "loss:0.172283 [19232/60000]\n",
            "loss:0.187076 [22432/60000]\n",
            "loss:0.219396 [25632/60000]\n",
            "loss:0.199454 [28832/60000]\n",
            "loss:0.187071 [32032/60000]\n",
            "loss:0.251817 [35232/60000]\n",
            "loss:0.207873 [38432/60000]\n",
            "loss:0.168603 [41632/60000]\n",
            "loss:0.180356 [44832/60000]\n",
            "loss:0.189913 [48032/60000]\n",
            "loss:0.183945 [51232/60000]\n",
            "loss:0.195112 [54432/60000]\n",
            "loss:0.186150 [57632/60000]\n",
            "train_loss = 0.20644534344673157, test_loss = 0.20653799690377597\n",
            "# Epoch 16\n",
            "loss:0.198778 [   32/60000]\n",
            "loss:0.195442 [ 3232/60000]\n",
            "loss:0.191314 [ 6432/60000]\n",
            "loss:0.196426 [ 9632/60000]\n",
            "loss:0.186108 [12832/60000]\n",
            "loss:0.247999 [16032/60000]\n",
            "loss:0.236609 [19232/60000]\n",
            "loss:0.202506 [22432/60000]\n",
            "loss:0.204057 [25632/60000]\n",
            "loss:0.216709 [28832/60000]\n",
            "loss:0.221817 [32032/60000]\n",
            "loss:0.165039 [35232/60000]\n",
            "loss:0.209656 [38432/60000]\n",
            "loss:0.233396 [41632/60000]\n",
            "loss:0.170992 [44832/60000]\n",
            "loss:0.236142 [48032/60000]\n",
            "loss:0.230765 [51232/60000]\n",
            "loss:0.187257 [54432/60000]\n",
            "loss:0.172808 [57632/60000]\n",
            "train_loss = 0.2064453435500463, test_loss = 0.2064832986924595\n",
            "# Epoch 17\n",
            "loss:0.221076 [   32/60000]\n",
            "loss:0.228469 [ 3232/60000]\n",
            "loss:0.172976 [ 6432/60000]\n",
            "loss:0.203896 [ 9632/60000]\n",
            "loss:0.213225 [12832/60000]\n",
            "loss:0.219215 [16032/60000]\n",
            "loss:0.234121 [19232/60000]\n",
            "loss:0.248098 [22432/60000]\n",
            "loss:0.236207 [25632/60000]\n",
            "loss:0.196364 [28832/60000]\n",
            "loss:0.177542 [32032/60000]\n",
            "loss:0.238727 [35232/60000]\n",
            "loss:0.216340 [38432/60000]\n",
            "loss:0.217205 [41632/60000]\n",
            "loss:0.218773 [44832/60000]\n",
            "loss:0.192486 [48032/60000]\n",
            "loss:0.183758 [51232/60000]\n",
            "loss:0.222517 [54432/60000]\n",
            "loss:0.184318 [57632/60000]\n",
            "train_loss = 0.20644534335136414, test_loss = 0.20645644818060696\n",
            "# Epoch 18\n",
            "loss:0.198275 [   32/60000]\n",
            "loss:0.220244 [ 3232/60000]\n",
            "loss:0.230823 [ 6432/60000]\n",
            "loss:0.207213 [ 9632/60000]\n",
            "loss:0.193689 [12832/60000]\n",
            "loss:0.194432 [16032/60000]\n",
            "loss:0.183434 [19232/60000]\n",
            "loss:0.185314 [22432/60000]\n",
            "loss:0.196748 [25632/60000]\n",
            "loss:0.182924 [28832/60000]\n",
            "loss:0.226215 [32032/60000]\n",
            "loss:0.228049 [35232/60000]\n",
            "loss:0.214868 [38432/60000]\n",
            "loss:0.174974 [41632/60000]\n",
            "loss:0.193059 [44832/60000]\n",
            "loss:0.199417 [48032/60000]\n",
            "loss:0.190318 [51232/60000]\n",
            "loss:0.193632 [54432/60000]\n",
            "loss:0.181929 [57632/60000]\n",
            "train_loss = 0.20644534358978273, test_loss = 0.20647427168326637\n",
            "# Epoch 19\n",
            "loss:0.212189 [   32/60000]\n",
            "loss:0.219643 [ 3232/60000]\n",
            "loss:0.209010 [ 6432/60000]\n",
            "loss:0.243422 [ 9632/60000]\n",
            "loss:0.241627 [12832/60000]\n",
            "loss:0.251600 [16032/60000]\n",
            "loss:0.212896 [19232/60000]\n",
            "loss:0.214140 [22432/60000]\n",
            "loss:0.243833 [25632/60000]\n",
            "loss:0.220488 [28832/60000]\n",
            "loss:0.217284 [32032/60000]\n",
            "loss:0.225811 [35232/60000]\n",
            "loss:0.248780 [38432/60000]\n",
            "loss:0.203952 [41632/60000]\n",
            "loss:0.179084 [44832/60000]\n",
            "loss:0.213380 [48032/60000]\n",
            "loss:0.209676 [51232/60000]\n",
            "loss:0.234944 [54432/60000]\n",
            "loss:0.195864 [57632/60000]\n",
            "train_loss = 0.2064453434944153, test_loss = 0.20648740865171147\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_9O97Hf02LWT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}

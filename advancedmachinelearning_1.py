"""AdvancedMachineLearning_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aA_3XseO77aoA-HS4AL1TWcovqsEdSf0

# AML 1

## Task 1
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install torchsummary

import torch
from torch import nn
from torch.utils.data import DataLoader
from torchvision import datasets
from torchvision.transforms import ToTensor

batch_size = 32

# Dataset import as in Ptytorch docs
ds_train = datasets.FashionMNIST(
    root="data",
    train=True,
    download=True,
    transform=ToTensor(),
)
ds_test = datasets.FashionMNIST(
    root="data",
    train=False,
    download=True,
    transform=ToTensor(),
)

train_loader = DataLoader(ds_train, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(ds_test, batch_size=batch_size, shuffle=False)

# Defining the labels from datasetg - taken from kaggle
labels = {
    0: "T-shirt",
    1: "Trouser",
    2: "Pullover",
    3: "Dress",
    4: "Coat",
    5: "Sandal",
    6: "Shirt",
    7: "Sneaker",
    8: "Bag",
    9: "Ankle Boot",
}

# Selecting the right device to perform the computations on
device = "cuda" if torch.cuda.is_available else "cpu"
print(f"Using {device} device")

# Definition of the model


class FashionMnistClassifier(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv_block_1 = nn.Sequential(
            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )

        self.drop1 = nn.Dropout2d(0.25)

        self.conv_block_2 = nn.Sequential(
            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )

        self.drop2 = nn.Dropout2d(0.25)

        self.flatten = nn.Flatten()

        self.linear_relu_stack = nn.Sequential(
            nn.Linear(7 * 7 * 64, 512), nn.ReLU(), nn.Linear(512, 512), nn.ReLU(), nn.Linear(512, 10)
        )

    def forward(self, x):
        out = self.conv_block_1(x)
        out = self.drop1(out)
        out = self.conv_block_2(out)
        out = self.drop2(out)
        out = self.linear_relu_stack(self.flatten(out))
        return out


model = FashionMnistClassifier().to(device)
print(model.parameters())

# Settings
learning_rate = 1e-3
epochs = 5

loss = nn.CrossEntropyLoss()  # nn.MSELoss()

optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)


def training_epoch(dataloader, model, loss, opt):
    model.train()  # Setting model into train mode
    samples = len(dataloader) * dataloader.batch_size
    processed = 0
    batch_num = 0

    for data, label in dataloader:
        data = data.to(device)
        label = label.to(device)

        opt.zero_grad()

        predictions = model(data)
        loss_result = loss(predictions, label)
        loss_result.backward()

        opt.step()

        processed += len(data)
        if batch_num % 100 == 0:
            cur_loss = loss_result.item()
            print(f"loss:{cur_loss:>7f} [{processed:>5d}/{samples:>5d}]")

        batch_num += 1


def evaluate(dataloader, model, loss):
    samples = len(dataloader.dataset)
    n_batches = len(dataloader)
    model.eval()  # Disenables storing training data

    test_loss = 0
    correct = 0

    with torch.no_grad():
        for data, label in dataloader:
            data = data.to(device)
            label = label.to(device)

            predictions = model(data)

            loss_val = loss(predictions, label).item()
            correct += (predictions.argmax(1) == label).type(torch.float).sum().item()
            test_loss += loss_val
    test_loss /= n_batches
    accuracy = correct / samples

    return accuracy, test_loss, correct  # , cm


#     correct = 0
#     with torch.no_grad():
#         for data, label in dataloader:
#             data, label = data.to(device), label.to(device)
#             predictions = model(data)
#             correct += (predictions.argmax(1) == label).type(torch.float).sum().item()
#     accuracy = correct / len(dataloader.dataset)


def run_training(train, test, model, loss, opt, epochs):

    for e in range(epochs):
        print(f"# Epoch {e}")
        training_epoch(train, model, loss, opt)
        train_accuracy, train_loss, train_correct = evaluate(train, model, loss)
        test_accuracy, test_loss, test_correct = evaluate(test, model, loss)
        print(
            f"train_loss = {train_loss}, train_acc = {train_accuracy}, test_loss = {test_loss}, test_acc = {test_accuracy}"
        )


from torchsummary import summary

summary(model, input_size=(1, 28, 28))

run_training(train_loader, test_loader, model, loss, optimizer, 20)

"""## Task 2"""


class AutoEncoder(torch.nn.Module):
    def __init__(self, input_size):
        super().__init__()
        self.encoder = torch.nn.Sequential(
            torch.nn.Linear(input_size, 128),
            torch.nn.ReLU(),
            torch.nn.Linear(128, 64),
            torch.nn.ReLU(),
            torch.nn.Linear(64, 32),
            torch.nn.ReLU(),
            torch.nn.Linear(32, 16),
        )

        self.decoder = torch.nn.Sequential(
            torch.nn.Linear(16, 32),
            torch.nn.ReLU(),
            torch.nn.Linear(32, 64),
            torch.nn.ReLU(),
            torch.nn.Linear(64, 128),
            torch.nn.ReLU(),
            torch.nn.Linear(128, input_size),
            torch.nn.ReLU(),
        )

    def forward(self, x):
        out = self.encoder(x)
        out = self.decoder(out)
        return out


ds_train = datasets.FashionMNIST(root="data", train=True, download=True, transform=ToTensor())
ds_test = datasets.FashionMNIST(root="data", train=False, download=True, transform=ToTensor())
batch_size = 32
train_dataloader = DataLoader(ds_train, batch_size=batch_size, shuffle=True)
test_dataloader = DataLoader(ds_test, batch_size=batch_size, shuffle=True)

model = AutoEncoder(28 * 28)


optimizer = torch.optim.Adam(model.parameters(), lr=1e-1, weight_decay=1e-8)

loss_f = torch.nn.MSELoss()

# Selecting the right device to perform the computations on
device = "cuda" if torch.cuda.is_available else "cpu"
print(f"Using {device} device")

model = model.to(device)


def one_epoch_train(model, optimizer, loss, train_dataloader, device):
    model.train()
    processed_images = 0
    current_batch_index = 0
    samples = len(train_dataloader.dataset)
    for image, _ in train_dataloader:
        image = image.to(device)
        image = image.reshape(-1, 28 * 28)

        reconstructed_image = model(image)
        computed_loss = loss(reconstructed_image, image)
        computed_loss.backward()

        optimizer.step()

        processed_images += len(image)
        if current_batch_index % 100 == 0:
            current_loss_value = computed_loss.item()
            print(f"loss:{current_loss_value:>7f} [{processed_images:>5d}/{samples:>5d}]")
        current_batch_index += 1


def evaluate(model, loss, dataloader, device):
    model.eval()
    samples = len(dataloader.dataset)
    n_batches = len(dataloader)
    total_loss = 0
    samples = len(dataloader.dataset)
    for image, _ in dataloader:
        image = image.to(device)
        image = image.reshape(-1, 28 * 28)

        reconstructed_image = model(image)
        computed_loss = loss(reconstructed_image, image)
        total_loss += computed_loss.item()
    return total_loss / n_batches


def run_training(train, test, model, loss, opt, epochs):

    for e in range(epochs):
        print(f"# Epoch {e}")
        one_epoch_train(model, opt, loss, train, device)
        train_loss = evaluate(model, loss, train, device)
        test_loss = evaluate(model, loss, test, device)
        print(f"train_loss = {train_loss}, test_loss = {test_loss}")


run_training(train_dataloader, test_dataloader, model, loss_f, optimizer, 20)
